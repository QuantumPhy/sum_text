import pandas as pd
import re
import numpy as np
import csv
import sys
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, chi2
from nltk.tokenize import sent_tokenize,word_tokenize
from collections import defaultdict
from string import punctuation


filepath=r"C:\Users\amdin\Desktop\Input\in\in\PGS_IN_dos.csv"
data=pd.read_csv(filepath,sep=";",encoding="iso-8859-1")

data

stemmer = SnowballStemmer("french")
words = stopwords.words("french")

data['cleaned_ Incident Description'] = data[' Incident Description'].apply(lambda x: " ".join([stemmer.stem(i) for i in re.sub("[^a-zA-Zéèêçà@]", " ", x).split() if i not in words]).lower())


data.to_csv("data.csv",sep=';',index=False)

data['word_token'] = data['cleaned_ Incident Description'].apply(lambda x: word_tokenize(x))

def most_frequent_words(original_sentences, frequent_words, num_sentences):
    # Lower the case to make matching simple
    sentences_lowered = [sentence.lower() for sentence in original_sentences]
    # a map of sentences with the highest concentration of frequent words
    sentences = defaultdict(int)
    # For the most frequent words
    for word in frequent_words:
        # While looking through our lowercased sentences
        for i in range(0, len(sentences_lowered)):
            # If we find the word in the sentence
            if word in sentences_lowered[i]:
                # Note the count in the map
                sentences[original_sentences[i]] += 1
    # Swap the sentence with the count
    sentences = dict(zip(sentences.values(), sentences.keys()))
    # sort the keys so we get the greatest frequency of words first
    keys = sorted(sentences.keys(), reverse=True)
    # Return the top num_sentences of high frequency worded sentences
    return [sentences[keys[i]] for i in range(0, num_sentences)]


def get_context(needle, haystack, context):
    # Locate the value in the original_sentences
    for i in range(0, len(haystack)):
        if haystack[i] == needle:
            prefix = i - context
            if prefix < 0:
                prefix = 0
            # Return the number of context sentences asked for
            return haystack[prefix:i+(context+1)]

def summarize(text):
    result = {}
    # Remove newlines from the text
    #text = re.sub('\s*[\n|\r|\r\n]\s*', ' ', text)
    # Separate all the words
    base_words = [word.lower() for word in word_tokenize(text)]
    # Remove non-nouns from the words collected
    words = [word for word in base_words if word not in non_nouns]
    # Use a frequency distribution to encode how often a word occurs
    frequencies = FreqDist(words)
    # Now create a set of the most frequent words, limit top 100 words
    frequent_words = [pair[0] for pair in frequencies.items()[:100]]
    # Separate all the sentences
    original_sentences = sentence_tokenizer.tokenize(text)
    # Find the sentences with the most frequent words
    high_freq_sentences = most_frequent_words(original_sentences,
                                              frequent_words, num_sentences)
    # Create a map with the High Frequence Sentences as the key,
    # and the context as the value
    for sentence in high_freq_sentences:
        # Get context for each of the high freq sentences
        result[sentence] = get_context(sentence,original_sentences, context_lines)
    return result
    
    
    data['word_token_sum'] = data['cleaned_ Incident Description'].apply(lambda x: summarize(x))
